# LLM Documentation Explorer

## Running Locally

- install ollama
- download llama3
- download repo
- download dropbox folder
- ingest documents
- run backend server
- run frontend server

## Resources:
- https://www.youtube.com/playlist?list=PLrSHiQgy4VjGQohoAmgX9VFH52psNOu71
- https://github.com/AI-Maker-Space/LLM-Ops-Cohort-1/tree/main


Backend: `poetry run uvicorn main:app --reload`
Frontend: `poetry run streamlit run main.py`



Streaming:
- https://discuss.streamlit.io/t/listening-for-updates-from-an-api-server/48486
- 