{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import shutil\n",
    "import getpass\n",
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "class Model(Enum):\n",
    "    OPEN_AI = 1\n",
    "    LLAMA = 2\n",
    "    NOMIC = 3\n",
    "\n",
    "QUERIES = [\n",
    "    \"How to deal with a memory leak in prod?\",\n",
    "    \"What are the responsibilities of the First Captain during a fire?\",\n",
    "    \"What are the First Captain's responsibilities during a fire?\",\n",
    "    \"What should I do if there are problems with the haskell quiz engine?\",\n",
    "    \"What are my responsibilites when on call?\",\n",
    "    \"What do I do if my time off conflicts with being on call?\",\n",
    "]\n",
    "\n",
    "DOWNLOAD_FOLDER = \"content\"\n",
    "TEST_CONTENT_DROPBOX_FOLDER = \"LLM Doc Exp Test Content\"\n",
    "EMBEDDINGS_MODEL = Model.NOMIC\n",
    "LLM_MODEL = Model.LLAMA\n",
    "RESET_DROPBOX = False\n",
    "RESET_VECTOR_STORE = False\n",
    "TEST_QUERY = QUERIES[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'How to deal with a memory leak in prod?', 'result': \"Based on the provided context, it appears that the memory leak issue was resolved by identifying and fixing the root cause in a specific pull request (https://github.com/NoRedInk/NoRedInk/pull/13550) and deploying the fix in https://github.com/NoRedInk/NoRedInk/pull/13703. The exact steps taken to resolve the issue are not specified, but it seems that a combination of investigation, testing, and deployment were involved.\\n\\nIt's worth noting that the context also mentions some potential solutions or workarounds that were tried, such as running free memory usage then re-running the application, disabling Sass, and increasing the thread pool. However, these attempts did not ultimately resolve the issue.\\n\\nIn general, dealing with a memory leak in production can be challenging and may require a combination of detective work, testing, and deployment to identify and fix the root cause.\", 'source_documents': [Document(page_content='# 2016‚Äì09‚Äì30 Other (~resolvedüéâ?) Fires\\n\\nMemory leak in prod\\n\\nUpdate / Tentatively Resolved\\nAppears that the cause was not in the suspect list below (but in the deploy) https://github.com/NoRedInk/NoRedInk/pull/13550 which is now the primary suspect with a fix deployed in https://github.com/NoRedInk/NoRedInk/pull/13703 that thus far appears to have removed network correlated memory spikes.\\n\\nThings we can do\\n\\nDeploy/rollback to production-2016-09-29-0434\\n\\nRevert the suspect PRs\\n\\nSuspect PRs\\nFrom @Joshua L (sorted by suspicion)', metadata={'source': '.content/Engineering/Fires/Old_Fires/2016‚Äì09‚Äì30_Other_(~resolved_)_Fires.md'}), Document(page_content='Solutions/thoughts\\n\\nFree memory usage then run again\\nseems to work sometimes, othertimes does not\\n\\nOccasionally, it will build successfully\\nunable to reproduce consistently\\n\\nWiping elm-stuff prior to running webpack has no success\\n\\nTried to increase thread pool via https://github.com/jtangelder/sass-loader/issues/100#issuecomment-104081891\\nDid not change anything\\n\\nGoing to try bumping sass-loaderversion\\nsass-loader ‚Üí Did not work\\n\\nDisabled sass\\nStill hitting the same problem\\nHints that the problem is elsewhere\\n\\nUsing --bail --profile --progress --display-chunks --optimize-min-chunk-size 10000 --optimize-max-chunks 1 --display-reasons', metadata={'source': '.content/Engineering/Fires/Old_Fires/2016-11-30_Webpack_breaking_and_blocking_deploys.md'}), Document(page_content='[ ] complete the backfill\\n    - ~~investigate backfilling in id order, not grammar_quiz_question_id order (JOSH)~~\\n    - ~~investigate how to solve~~ ~~backfill contention problems~~\\n        - ~~Option 1:~~\\n            [ ] ~~Move select to replica~~\\n        - Option 2:  ‚Üê WENT WITH OPTION 2\\n            [x] set innodb_autoinc_lock_mode to 2 instead of the default value 1 (from https://dev.mysql.com/doc/refman/5.5/en/optimizing-innodb-bulk-data-loading.html)\\n            - apply type: static, which needs a reboot but won‚Äôt be a problem if we‚Äôre multi-az\\n            [x] go back to insert into ‚Ä¶ select  ‚Üê MORE THAN 2‚Äì10x FASTER\\n        - ~~Option 3 (if we still have contention)~~\\n            [ ] ~~ALTER TABLE on the old tables (Thanks @jeg)~~\\n            [ ] ~~when alter table is completed, add trigger to send new data & modifications when they come~~\\n            [ ] ~~backfill the gap~~\\n            [ ] ~~When the backfill is over, promote~~ ~~_old~~ ~~~~ ~~tables.~~', metadata={'source': '.content/Engineering/Fires/Old_Fires/2016‚Äì09‚Äì28_Next_Steps_after_Live_Restore.md'}), Document(page_content='we can run the executable manually\\nthe readiness endpoint says that postgres is down\\n\\nthere‚Äôs no log output on crashes!\\nwe modify /etc/systemd/system/drafts.service by commenting out the StandardOutput and StandardError stanzas that are suppressing the output from being sent to syslog (or whatever the inherited output sink is)\\n‚ö†Ô∏è we had to run systemctl daemon-reload to reload the config.\\nBrian‚Äôs note: I almost forgot this command at first! It‚Äôs super weird that systemd manages things this way (IIRC it‚Äôs because of timers and triggers) and I suspect it‚Äôs not common knowledge that you need to do this after updating a unit. If we had not run this, the changes would just not be picked up and we would not have been warned that there was an un-HUP‚Äôd systemd daemon just ignoring our changes.\\n\\n23:10\\n\\nwe realize that an env variable, LOG_ENABLED_LOGGERS actually changed wrt the last deploy', metadata={'source': '.content/Engineering/Fires/Old_Fires/2020-04-22_Unresponsive_Drafts_on_Demo.md'})]}"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# import sseclient\n",
    "# import json\n",
    "\n",
    "\n",
    "# def get_stream(app, query):\n",
    "#     full_response = \"\"\n",
    "#     url = \"http://127.0.0.1:8000/query/stream_events/\"\n",
    "#     response = requests.post(url, json={\"input\": {\"query\": query}}, stream=True)\n",
    "#     client = sseclient.SSEClient(response)\n",
    "#     for event in client.events():\n",
    "#         output = json.loads(event.data)\n",
    "#         if output[\"event\"] == \"on_llm_stream\":\n",
    "#             if chunk := output[\"data\"].get(\"chunk\"):\n",
    "#                 full_response += chunk\n",
    "#                 yield full_response\n",
    "#         elif output[\"event\"] == \"on_chain_end\":\n",
    "#             if source_documents := output[\"data\"][\"output\"].get(\"source_documents\"):\n",
    "#                 yield output[\"data\"][\"output\"][\"result\"]\n",
    "\n",
    "# for chunk in get_stream(None, TEST_QUERY):\n",
    "#     print(chunk, flush=True)\n",
    "\n",
    "\n",
    "# from langserve import RemoteRunnable\n",
    "# query_chain = RemoteRunnable(\"http://127.0.0.1:8000/query/\")\n",
    "\n",
    "# async for msg in query_chain.astream({\"query\": QUERIES[0]}):\n",
    "#     print(msg, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = {\n",
    "    Model.OPEN_AI: OpenAIEmbeddings,\n",
    "    Model.LLAMA: lambda: OllamaEmbeddings(model=\"llama3\"),\n",
    "    Model.NOMIC: lambda: OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llms = {\n",
    "    Model.OPEN_AI: lambda: ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "    Model.LLAMA: lambda: Ollama(model=\"llama3\", temperature=0),\n",
    "    Model.NOMIC: lambda: Ollama(model=\"llama3\", temperature=0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESET_DROPBOX:\n",
    "    dbx = dropbox.Dropbox(getpass.getpass(\"Dropbox API Key:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure local download folder exists, and delete its contents\n",
    "\n",
    "def create_download_folder():\n",
    "    if not os.path.exists(DOWNLOAD_FOLDER):\n",
    "        os.makedirs(DOWNLOAD_FOLDER)\n",
    "\n",
    "def clear_downloads_folder():\n",
    "    for filename in os.listdir(DOWNLOAD_FOLDER):\n",
    "        file_path = os.path.join(DOWNLOAD_FOLDER, filename)\n",
    "        print(file_path)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "\n",
    "create_download_folder()\n",
    "if RESET_DROPBOX:\n",
    "    clear_downloads_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dropbox paper docs and download to content file\n",
    "\n",
    "Download selected test paper docs using the [Dropbox API](https://www.dropbox.com/developers/documentation/http/documentation#paper-docs-download) and [Python SDK](https://dropbox-sdk-python.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "The dashboard for the Dropbox App used to do this can be found [here](https://www.dropbox.com/developers/apps/info/la3hq2wkhl5wx4m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dropbox.paper import ExportFormat\n",
    "# folders_to_match = set([\"Engineering\", \"Fires\"])\n",
    "\n",
    "# print(\"Getting doc ids\")\n",
    "# doc_ids = dbx.paper_docs_list().doc_ids\n",
    "\n",
    "# print(\"Getting docs in matching folders\")\n",
    "# docs_ids_in_folder = [doc_id for doc_id in doc_ids if folders_to_match.issubset(set([folder.name for folder in dbx.paper_docs_get_folder_info(doc_id).folders or []]))]\n",
    "\n",
    "# print(\"Getting doc titles\")\n",
    "# doc_titles = {doc_id: dbx.paper_docs_download(doc_id, ExportFormat('markdown'))[0].title for doc_id in docs_ids_in_folder}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dropbox.paper import ExportFormat, ListPaperDocsFilterBy\n",
    "\n",
    "def get_file_path(doc_id):\n",
    "    return os.path.join(DOWNLOAD_FOLDER, f\"{doc_titles[doc_id]}.md\")\n",
    "\n",
    "def download_doc(doc_id):\n",
    "    result = dbx.paper_docs_download_to_file(get_file_path(doc_id), doc_id, ExportFormat('markdown'))\n",
    "    print(f\"- downloaded '{result.title}'\")\n",
    "    return result\n",
    "\n",
    "if RESET_DROPBOX:\n",
    "    print(\"Retrieving document IDs\")\n",
    "    doc_ids = dbx.paper_docs_list(filter_by=ListPaperDocsFilterBy.docs_created).doc_ids\n",
    "    print(f\"- {len(doc_ids)} documents found\")\n",
    "\n",
    "    print(\"Filtering documents in folder\")\n",
    "    docs_ids_in_folder = [doc_id for doc_id in doc_ids if TEST_CONTENT_DROPBOX_FOLDER in [folder.name for folder in dbx.paper_docs_get_folder_info(doc_id).folders or []]]\n",
    "    print(f\"- {len(docs_ids_in_folder)} documents found in folder\")\n",
    "\n",
    "    print(\"Retrieving document titles\")\n",
    "    doc_titles = {doc_id: dbx.paper_docs_download(doc_id, ExportFormat('markdown'))[0].title for doc_id in docs_ids_in_folder}\n",
    "\n",
    "    print(\"Downloading documents\")\n",
    "    results = [download_doc(doc_id) for doc_id in docs_ids_in_folder]\n",
    "    print(\"Download complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG Q&A using the downloaded files\n",
    "\n",
    "Uses [this repo](https://github.com/AI-Maker-Space/LLM-Ops-Cohort-1/blob/main/Week%201/Tuesday/Barbie_Retrieval_Augmented_Question_Answering_(RAQA)_Assignment%20(Assignment%20Version).ipynb) as a reference.\n",
    "\n",
    "\n",
    "Additional Resources:\n",
    "https://github.com/zylon-ai/private-gpt/issues/358#issuecomment-1563663500\n",
    "https://python.langchain.com/docs/integrations/vectorstores/starrocks/\n",
    "https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_text_splitters import Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(DOWNLOAD_FOLDER, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "raw_documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# headers_to_split_on = [\n",
    "#     (\"#\", \"Header 1\"),\n",
    "#     (\"##\", \"Header 2\"),\n",
    "# ]\n",
    "\n",
    "# # MD splits\n",
    "# markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "#     headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "# )\n",
    "\n",
    "# loader.load_and_split(text_splitter=markdown_splitter)\n",
    "# MarkdownTextSplitter()\n",
    "\n",
    "# # Char-level splits\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# chunk_size = 250\n",
    "# chunk_overlap = 30\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "# )\n",
    "\n",
    "# # Split\n",
    "# splits = text_splitter.split_documents(md_header_splits)\n",
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text_splitter = MarkdownTextSplitter(\n",
    "#     chunk_size = 1000, # the character length of the chunk\n",
    "#     chunk_overlap = 100, # the character length of the overlap between chunks\n",
    "#     length_function = len, # the length function\n",
    "# )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(Language.MARKDOWN, \n",
    "    chunk_size = 1000, # the character length of the chunk\n",
    "    chunk_overlap = 100, # the character length of the overlap between chunks\n",
    "    length_function = len, # the length function\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Creation\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/vectorstores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.storage import LocalFileStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "core_embeddings_model = embeddings[EMBEDDINGS_MODEL]()\n",
    "\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    core_embeddings_model, store, namespace=core_embeddings_model.model\n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embedder)\n",
    "vector_store.save_local(\"./vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query on the vector store\n",
    "\n",
    "query = TEST_QUERY\n",
    "embedding_vector = core_embeddings_model.embed_query(query)\n",
    "docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)\n",
    "\n",
    "for page in docs:\n",
    "  print(f\">>>{page.page_content}<<<\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms[LLM_MODEL]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    callbacks=[handler],\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = qa_with_sources_chain({\"query\" : TEST_QUERY})\n",
    "result = qa_with_sources_chain.invoke(TEST_QUERY)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "RESPONSE_TEMPLATE = \"\"\"\\\n",
    "You are an expert programmer and problem-solver, tasked with answering any question \\\n",
    "about Langchain.\n",
    "\n",
    "Generate a comprehensive and informative answer of 80 words or less for the \\\n",
    "given question based solely on the provided search results (URL and content). You must \\\n",
    "only use information from the provided search results. Use an unbiased and \\\n",
    "journalistic tone. Combine search results together into a coherent answer. Do not \\\n",
    "repeat text. Cite search results using [${{number}}] notation. Only cite the most \\\n",
    "relevant results that answer the question accurately. Place these citations at the end \\\n",
    "of the sentence or paragraph that reference them - do not put them all at the end. If \\\n",
    "different results refer to different entities within the same name, write separate \\\n",
    "answers for each entity.\n",
    "\n",
    "You should use bullet points in your answer for readability. Put citations where they apply\n",
    "rather than putting them all at the end.\n",
    "\n",
    "If there is nothing in the context relevant to the question at hand, just say \"Hmm, \\\n",
    "I'm not sure.\" Don't try to make up an answer.\n",
    "\n",
    "Anything between the following `context`  html blocks is retrieved from a knowledge \\\n",
    "bank, not part of the conversation with the user. \n",
    "\n",
    "<context>\n",
    "    {context} \n",
    "<context/>\n",
    "\n",
    "REMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm \\\n",
    "not sure.\" Don't try to make up an answer. Anything between the preceding 'context' \\\n",
    "html blocks is retrieved from a knowledge bank, not part of the conversation with the \\\n",
    "user.\\\n",
    "\"\"\"\n",
    "\n",
    "REPHRASE_TEMPLATE = \"\"\"\\\n",
    "Given the following conversation and a follow up question, rephrase the follow up \\\n",
    "question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone Question:\"\"\"\n",
    "\n",
    "def format_docs(docs) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(doc_string)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "def serialize_history(request):\n",
    "    chat_history = request[\"chat_history\"] or []\n",
    "    converted_chat_history = []\n",
    "    for message in chat_history:\n",
    "        if message.get(\"human\") is not None:\n",
    "            converted_chat_history.append(HumanMessage(content=message[\"human\"]))\n",
    "        if message.get(\"ai\") is not None:\n",
    "            converted_chat_history.append(AIMessage(content=message[\"ai\"]))\n",
    "    return converted_chat_history\n",
    "\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)\n",
    "condense_question_chain = (\n",
    "    CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()\n",
    ").with_config(\n",
    "    run_name=\"CondenseQuestion\",\n",
    ")\n",
    "conversation_chain = condense_question_chain | retriever\n",
    "retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),\n",
    "        conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\n",
    "    ),\n",
    "    (\n",
    "        RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "            run_name=\"Itemgetter:question\"\n",
    "        )\n",
    "        | retriever\n",
    "    ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\n",
    ").with_config(run_name=\"RouteDependingOnChatHistory\")\n",
    "\n",
    "\n",
    "context = (\n",
    "    RunnablePassthrough.assign(docs=retriever_chain)\n",
    "    .assign(context=lambda x: format_docs(x[\"docs\"]))\n",
    "    .with_config(run_name=\"RetrieveDocs\")\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", RESPONSE_TEMPLATE),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "default_response_synthesizer = prompt | llm\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=serialize_history) | context | default_response_synthesizer\n",
    "\n",
    "chain.run({\"question\": \"How to deal with a memory leak in prod?\", \"chat_history\": []})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(f\"{result['query']}\\n\")\n",
    "pprint.pp(result['result'])\n",
    "print()\n",
    "\n",
    "pprint.pp([document.metadata['source'] for document in result['source_documents']])\n",
    "# pprint.pp(result['source_documents'][0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-documentation-explorer-4idgME2U-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
