{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "import shutil\n",
    "import getpass\n",
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "class Model(Enum):\n",
    "    OPEN_AI = 1\n",
    "    LLAMA = 2\n",
    "    NOMIC = 3\n",
    "\n",
    "QUERIES = [\n",
    "    \"How to deal with a memory leak in prod?\",\n",
    "    \"What are the responsibilities of the First Captain during a fire?\",\n",
    "    \"What are the First Captain's responsibilities during a fire?\",\n",
    "    \"What should I do if there are problems with the haskell quiz engine?\",\n",
    "    \"What are my responsibilites when on call?\",\n",
    "    \"What do I do if my time off conflicts with being on call?\",\n",
    "]\n",
    "\n",
    "DOWNLOAD_FOLDER = \".content\"\n",
    "TEST_CONTENT_DROPBOX_FOLDER = \"LLM Doc Exp Test Content\"\n",
    "EMBEDDINGS_MODEL = Model.NOMIC\n",
    "LLM_MODEL = Model.LLAMA\n",
    "RESET_DROPBOX = False\n",
    "RESET_VECTOR_STORE = False\n",
    "TEST_QUERY = QUERIES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import sseclient\n",
    "# import json\n",
    "\n",
    "\n",
    "# def get_stream(app, query):\n",
    "#     full_response = \"\"\n",
    "#     url = \"http://127.0.0.1:8000/query/stream_events/\"\n",
    "#     response = requests.post(url, json={\"input\": {\"query\": query}}, stream=True)\n",
    "#     client = sseclient.SSEClient(response)\n",
    "#     for event in client.events():\n",
    "#         output = json.loads(event.data)\n",
    "#         if output[\"event\"] == \"on_llm_stream\":\n",
    "#             if chunk := output[\"data\"].get(\"chunk\"):\n",
    "#                 full_response += chunk\n",
    "#                 yield full_response\n",
    "#         elif output[\"event\"] == \"on_chain_end\":\n",
    "#             if source_documents := output[\"data\"][\"output\"].get(\"source_documents\"):\n",
    "#                 yield output[\"data\"][\"output\"][\"result\"]\n",
    "\n",
    "# for chunk in get_stream(None, TEST_QUERY):\n",
    "#     print(chunk, flush=True)\n",
    "\n",
    "\n",
    "# from langserve import RemoteRunnable\n",
    "# query_chain = RemoteRunnable(\"http://127.0.0.1:8000/query/\")\n",
    "\n",
    "# async for msg in query_chain.astream({\"query\": QUERIES[0]}):\n",
    "#     print(msg, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = {\n",
    "    Model.OPEN_AI: OpenAIEmbeddings,\n",
    "    Model.LLAMA: lambda: OllamaEmbeddings(model=\"llama3\"),\n",
    "    Model.NOMIC: lambda: OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llms = {\n",
    "    Model.OPEN_AI: lambda: ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0),\n",
    "    Model.LLAMA: lambda: Ollama(model=\"llama3\", temperature=0),\n",
    "    Model.NOMIC: lambda: Ollama(model=\"llama3\", temperature=0),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RESET_DROPBOX:\n",
    "    dbx = dropbox.Dropbox(getpass.getpass(\"Dropbox API Key:\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure local download folder exists, and delete its contents\n",
    "\n",
    "def create_download_folder():\n",
    "    if not os.path.exists(DOWNLOAD_FOLDER):\n",
    "        os.makedirs(DOWNLOAD_FOLDER)\n",
    "\n",
    "def clear_downloads_folder():\n",
    "    for filename in os.listdir(DOWNLOAD_FOLDER):\n",
    "        file_path = os.path.join(DOWNLOAD_FOLDER, filename)\n",
    "        print(file_path)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "\n",
    "create_download_folder()\n",
    "if RESET_DROPBOX:\n",
    "    clear_downloads_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter dropbox paper docs and download to content file\n",
    "\n",
    "Download selected test paper docs using the [Dropbox API](https://www.dropbox.com/developers/documentation/http/documentation#paper-docs-download) and [Python SDK](https://dropbox-sdk-python.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "The dashboard for the Dropbox App used to do this can be found [here](https://www.dropbox.com/developers/apps/info/la3hq2wkhl5wx4m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dropbox.paper import ExportFormat\n",
    "# folders_to_match = set([\"Engineering\", \"Fires\"])\n",
    "\n",
    "# print(\"Getting doc ids\")\n",
    "# doc_ids = dbx.paper_docs_list().doc_ids\n",
    "\n",
    "# print(\"Getting docs in matching folders\")\n",
    "# docs_ids_in_folder = [doc_id for doc_id in doc_ids if folders_to_match.issubset(set([folder.name for folder in dbx.paper_docs_get_folder_info(doc_id).folders or []]))]\n",
    "\n",
    "# print(\"Getting doc titles\")\n",
    "# doc_titles = {doc_id: dbx.paper_docs_download(doc_id, ExportFormat('markdown'))[0].title for doc_id in docs_ids_in_folder}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dropbox.paper import ExportFormat, ListPaperDocsFilterBy\n",
    "\n",
    "def get_file_path(doc_id):\n",
    "    return os.path.join(DOWNLOAD_FOLDER, f\"{doc_titles[doc_id]}.md\")\n",
    "\n",
    "def download_doc(doc_id):\n",
    "    result = dbx.paper_docs_download_to_file(get_file_path(doc_id), doc_id, ExportFormat('markdown'))\n",
    "    print(f\"- downloaded '{result.title}'\")\n",
    "    return result\n",
    "\n",
    "if RESET_DROPBOX:\n",
    "    print(\"Retrieving document IDs\")\n",
    "    doc_ids = dbx.paper_docs_list(filter_by=ListPaperDocsFilterBy.docs_created).doc_ids\n",
    "    print(f\"- {len(doc_ids)} documents found\")\n",
    "\n",
    "    print(\"Filtering documents in folder\")\n",
    "    docs_ids_in_folder = [doc_id for doc_id in doc_ids if TEST_CONTENT_DROPBOX_FOLDER in [folder.name for folder in dbx.paper_docs_get_folder_info(doc_id).folders or []]]\n",
    "    print(f\"- {len(docs_ids_in_folder)} documents found in folder\")\n",
    "\n",
    "    print(\"Retrieving document titles\")\n",
    "    doc_titles = {doc_id: dbx.paper_docs_download(doc_id, ExportFormat('markdown'))[0].title for doc_id in docs_ids_in_folder}\n",
    "\n",
    "    print(\"Downloading documents\")\n",
    "    results = [download_doc(doc_id) for doc_id in docs_ids_in_folder]\n",
    "    print(\"Download complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple RAG Q&A using the downloaded files\n",
    "\n",
    "Uses [this repo](https://github.com/AI-Maker-Space/LLM-Ops-Cohort-1/blob/main/Week%201/Tuesday/Barbie_Retrieval_Augmented_Question_Answering_(RAQA)_Assignment%20(Assignment%20Version).ipynb) as a reference.\n",
    "\n",
    "\n",
    "Additional Resources:\n",
    "https://github.com/zylon-ai/private-gpt/issues/358#issuecomment-1563663500\n",
    "https://python.langchain.com/docs/integrations/vectorstores/starrocks/\n",
    "https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_text_splitters import Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(DOWNLOAD_FOLDER, glob=\"**/*.md\", loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "raw_documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# headers_to_split_on = [\n",
    "#     (\"#\", \"Header 1\"),\n",
    "#     (\"##\", \"Header 2\"),\n",
    "# ]\n",
    "\n",
    "# # MD splits\n",
    "# markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "#     headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    "# )\n",
    "\n",
    "# loader.load_and_split(text_splitter=markdown_splitter)\n",
    "# MarkdownTextSplitter()\n",
    "\n",
    "# # Char-level splits\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# chunk_size = 250\n",
    "# chunk_overlap = 30\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "# )\n",
    "\n",
    "# # Split\n",
    "# splits = text_splitter.split_documents(md_header_splits)\n",
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text_splitter = MarkdownTextSplitter(\n",
    "#     chunk_size = 1000, # the character length of the chunk\n",
    "#     chunk_overlap = 100, # the character length of the overlap between chunks\n",
    "#     length_function = len, # the length function\n",
    "# )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_language(Language.MARKDOWN, \n",
    "    chunk_size = 1000, # the character length of the chunk\n",
    "    chunk_overlap = 100, # the character length of the overlap between chunks\n",
    "    length_function = len, # the length function\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='🔥 Fire triage drill\\n\\nNote: This document is not intended to be consumed by itself. I’m personally introducing fire fighters to this drill at the start of their on-call cycle, and the document is here for a) reference for me b) reference for fire fighters after our meeting.\\n\\nThe fire drill starts at the NewRelic APM main screen for the Monolith. Go through the numbers on the image, reading their description below, and follow the arrows ➡️ .\\n\\nNewRelic\\n\\nMonolith NewRelic main screen\\n\\n🔗 Link\\n\\nRequest breakdown\\nDid something start taking up a larger share of request time?\\nExternal services\\n➡️ Go to the External Services tab in the right and figure out which service\\n\\n\\nDB\\n➡️ Go to the Databases tab and see whether some transactions started spiking\\n➡️ Open RDS Performance Insights for higher resolution data\\n\\n\\nQueueing or Ruby Slowness\\n➡️ Open Load Balancers Dashboard\\n➡️ Opsworks time-based instances and load-based instances\\nDo we have instances booting up?' metadata={'source': '.content/Engineering/Fires/Fire_triage_drill.md'}\n",
      "page_content='Throughput\\nDo we have a huge spike in throughput? 200k RPM and above might mean we’re under a DDoS attack\\n➡️ Check out what to do here: +Firefighting Resources: Fighting-a-DDoS-attack\\n\\nError rate\\nAre error spiking? Like 1% or more\\n➡️ Go to Errors tab\\n➡️ Check out Bugsnag (finer grained timeline)\\n\\nExternal Services\\n\\nServices by total response time\\nAny service started taking a longer time than usual?\\nA Haskell service?\\n➡️ Go to the specific service’s dashboard in NewRelic and do a similar analysis there (what made it slow? etc)\\n➡️ Also, open the Haskell Services in Kubernete dashboard in Datadog, because Kubernetes might be at fault\\nWhat services are in Kubernetes?\\nDrafts\\nQuiz Engine\\nTutorials' metadata={'source': '.content/Engineering/Fires/Fire_triage_drill.md'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Creation\n",
    "\n",
    "https://python.langchain.com/docs/modules/data_connection/vectorstores/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.storage import LocalFileStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = LocalFileStore(\".cache\")\n",
    "\n",
    "core_embeddings_model = embeddings[EMBEDDINGS_MODEL]()\n",
    "\n",
    "embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    core_embeddings_model, store, namespace=core_embeddings_model.model\n",
    ")\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, core_embeddings_model, normalize_L2=True)\n",
    "vector_store.save_local(\".vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistanceStrategy.EUCLIDEAN_DISTANCE\n",
      "{'page_content': '# 2016–09–30 Other (~resolved🎉?) Fires\\n\\nMemory leak in prod\\n\\nUpdate / Tentatively Resolved\\nAppears that the cause was not in the suspect list below (but in the deploy) https://github.com/NoRedInk/NoRedInk/pull/13550 which is now the primary suspect with a fix deployed in https://github.com/NoRedInk/NoRedInk/pull/13703 that thus far appears to have removed network correlated memory spikes.\\n\\nThings we can do\\n\\nDeploy/rollback to production-2016-09-29-0434\\n\\nRevert the suspect PRs\\n\\nSuspect PRs\\nFrom @Joshua L (sorted by suspicion)', 'metadata': {'source': '.content/Engineering/Fires/Old_Fires/2016–09–30_Other_(~resolved_)_Fires.md'}, 'type': 'Document'}\n",
      "{'page_content': 'Solutions/thoughts\\n\\nFree memory usage then run again\\nseems to work sometimes, othertimes does not\\n\\nOccasionally, it will build successfully\\nunable to reproduce consistently\\n\\nWiping elm-stuff prior to running webpack has no success\\n\\nTried to increase thread pool via https://github.com/jtangelder/sass-loader/issues/100#issuecomment-104081891\\nDid not change anything\\n\\nGoing to try bumping sass-loaderversion\\nsass-loader → Did not work\\n\\nDisabled sass\\nStill hitting the same problem\\nHints that the problem is elsewhere\\n\\nUsing --bail --profile --progress --display-chunks --optimize-min-chunk-size 10000 --optimize-max-chunks 1 --display-reasons', 'metadata': {'source': '.content/Engineering/Fires/Old_Fires/2016-11-30_Webpack_breaking_and_blocking_deploys.md'}, 'type': 'Document'}\n",
      "{'page_content': '--new-db-instance-identifier prod-internal-replica-old \\\\\\n      --apply-immediately \\\\\\n      --region \"us-west-2\"\\n    [x] rename prod-internal-replica-2 to prod-internal-replica\\n    aws-vault exec sudo -- aws rds modify-db-instance \\\\\\n      --db-instance-identifier prod-internal-replica2 \\\\\\n      --new-db-instance-identifier prod-internal-replica \\\\\\n      --apply-immediately \\\\\\n      --region \"us-west-2\"', 'metadata': {'source': '.content/Engineering/Fires/Old_Fires/Replication_Stopped.md'}, 'type': 'Document'}\n",
      "{'page_content': '[ ] complete the backfill\\n    - ~~investigate backfilling in id order, not grammar_quiz_question_id order (JOSH)~~\\n    - ~~investigate how to solve~~ ~~backfill contention problems~~\\n        - ~~Option 1:~~\\n            [ ] ~~Move select to replica~~\\n        - Option 2:  ← WENT WITH OPTION 2\\n            [x] set innodb_autoinc_lock_mode to 2 instead of the default value 1 (from https://dev.mysql.com/doc/refman/5.5/en/optimizing-innodb-bulk-data-loading.html)\\n            - apply type: static, which needs a reboot but won’t be a problem if we’re multi-az\\n            [x] go back to insert into … select  ← MORE THAN 2–10x FASTER\\n        - ~~Option 3 (if we still have contention)~~\\n            [ ] ~~ALTER TABLE on the old tables (Thanks @jeg)~~\\n            [ ] ~~when alter table is completed, add trigger to send new data & modifications when they come~~\\n            [ ] ~~backfill the gap~~\\n            [ ] ~~When the backfill is over, promote~~ ~~_old~~ ~~~~ ~~tables.~~', 'metadata': {'source': '.content/Engineering/Fires/Old_Fires/2016–09–28_Next_Steps_after_Live_Restore.md'}, 'type': 'Document'}\n"
     ]
    }
   ],
   "source": [
    "# Example query on the vector store\n",
    "print(vector_store.distance_strategy)\n",
    "\n",
    "query = TEST_QUERY\n",
    "embedding_vector = core_embeddings_model.embed_query(query)\n",
    "# docs = vector_store.similarity_search_by_vector(embedding_vector, k = 4)\n",
    "docs = vector_store.similarity_search_with_score_by_vector(embedding_vector, score_threshold=10.0)\n",
    "# docs = vector_store.similarity_search_with_score_by_vector(embedding_vector, k=4)\n",
    "\n",
    "for page in docs:\n",
    "  print(page[0].dict())\n",
    "  # print(f\">>>{page.page_content}<<<\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms[LLM_MODEL]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    store = LocalFileStore(\".cache\")\n",
    "\n",
    "    return CacheBackedEmbeddings.from_bytes_store(\n",
    "        core_embeddings_model, store, namespace=core_embeddings_model.model\n",
    "    )\n",
    "def load_vector_store():\n",
    "    embeddings = get_embeddings()\n",
    "    return FAISS.load_local(\n",
    "        \".vector_store\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "loaded_vector_store = load_vector_store()\n",
    "# retriever = vector_store.as_retriever()\n",
    "retriever = loaded_vector_store.as_retriever(\n",
    "    # search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\"score_threshold\": 0.9},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "qa_with_sources_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    callbacks=[handler],\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain.chains.qa_with_sources.retrieval\n",
    "import langchain.chains.retrieval_qa.base\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'What are the responsibilities of the First Captain during a fire?', 'result': \"I don't know.\", 'source_documents': []}\n"
     ]
    }
   ],
   "source": [
    "result = qa_with_sources_chain({\"query\" : QUERIES[1]})\n",
    "# from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# def get_context_from_documents(documents):\n",
    "#     document_separator = \"\\n\\n\"\n",
    "#     return document_separator.join(f\"Document {i}:\\n{doc.page_content}\" for i, doc in enumerate(documents))\n",
    "\n",
    "\n",
    "# context_chain = RunnablePassthrough.assign(context=lambda x: get_context_from_documents(x[\"source_documents\"]))\n",
    "# chain = qa_with_sources_chain | context_chain\n",
    "# result = chain.invoke(TEST_QUERY)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, MessagesPlaceholder, ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "RESPONSE_TEMPLATE = \"\"\"\\\n",
    "You are an expert programmer and problem-solver, tasked with answering any question \\\n",
    "about Langchain.\n",
    "\n",
    "Generate a comprehensive and informative answer of 80 words or less for the \\\n",
    "given question based solely on the provided search results (URL and content). You must \\\n",
    "only use information from the provided search results. Use an unbiased and \\\n",
    "journalistic tone. Combine search results together into a coherent answer. Do not \\\n",
    "repeat text. Cite search results using [${{number}}] notation. Only cite the most \\\n",
    "relevant results that answer the question accurately. Place these citations at the end \\\n",
    "of the sentence or paragraph that reference them - do not put them all at the end. If \\\n",
    "different results refer to different entities within the same name, write separate \\\n",
    "answers for each entity.\n",
    "\n",
    "You should use bullet points in your answer for readability. Put citations where they apply\n",
    "rather than putting them all at the end.\n",
    "\n",
    "If there is nothing in the context relevant to the question at hand, just say \"Hmm, \\\n",
    "I'm not sure.\" Don't try to make up an answer.\n",
    "\n",
    "Anything between the following `context`  html blocks is retrieved from a knowledge \\\n",
    "bank, not part of the conversation with the user. \n",
    "\n",
    "<context>\n",
    "    {context} \n",
    "<context/>\n",
    "\n",
    "REMEMBER: If there is no relevant information within the context, just say \"Hmm, I'm \\\n",
    "not sure.\" Don't try to make up an answer. Anything between the preceding 'context' \\\n",
    "html blocks is retrieved from a knowledge bank, not part of the conversation with the \\\n",
    "user.\\\n",
    "\"\"\"\n",
    "\n",
    "REPHRASE_TEMPLATE = \"\"\"\\\n",
    "Given the following conversation and a follow up question, rephrase the follow up \\\n",
    "question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone Question:\"\"\"\n",
    "\n",
    "def format_docs(docs) -> str:\n",
    "    formatted_docs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_string = f\"<doc id='{i}'>{doc.page_content}</doc>\"\n",
    "        formatted_docs.append(doc_string)\n",
    "    return \"\\n\".join(formatted_docs)\n",
    "\n",
    "def serialize_history(request):\n",
    "    chat_history = request[\"chat_history\"] or []\n",
    "    converted_chat_history = []\n",
    "    for message in chat_history:\n",
    "        if message.get(\"human\") is not None:\n",
    "            converted_chat_history.append(HumanMessage(content=message[\"human\"]))\n",
    "        if message.get(\"ai\") is not None:\n",
    "            converted_chat_history.append(AIMessage(content=message[\"ai\"]))\n",
    "    return converted_chat_history\n",
    "\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)\n",
    "condense_question_chain = (\n",
    "    CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()\n",
    ").with_config(\n",
    "    run_name=\"CondenseQuestion\",\n",
    ")\n",
    "conversation_chain = condense_question_chain | retriever\n",
    "retriever_chain = RunnableBranch(\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),\n",
    "        conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\n",
    "    ),\n",
    "    (\n",
    "        RunnableLambda(itemgetter(\"question\")).with_config(\n",
    "            run_name=\"Itemgetter:question\"\n",
    "        )\n",
    "        | retriever\n",
    "    ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\n",
    ").with_config(run_name=\"RouteDependingOnChatHistory\")\n",
    "\n",
    "\n",
    "context = (\n",
    "    RunnablePassthrough.assign(docs=retriever_chain)\n",
    "    .assign(context=lambda x: format_docs(x[\"docs\"]))\n",
    "    .with_config(run_name=\"RetrieveDocs\")\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", RESPONSE_TEMPLATE),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "default_response_synthesizer = prompt | llm\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=serialize_history) | context | default_response_synthesizer\n",
    "\n",
    "chain.run({\"question\": \"How to deal with a memory leak in prod?\", \"chat_history\": []})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(f\"{result['query']}\\n\")\n",
    "pprint.pp(result['result'])\n",
    "print()\n",
    "\n",
    "pprint.pp([document.metadata['source'] for document in result['source_documents']])\n",
    "# pprint.pp(result['source_documents'][0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-documentation-explorer-4idgME2U-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
